{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbc137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded: (2512, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ExtraTreeRegressor from version 1.5.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator IsolationForest from version 1.5.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.5.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Channel\n- CustomerAge\n- MerchantID\n- TransactionType\nFeature names seen at fit time, yet now missing:\n- category\n- category_amount_zscore\n- category_usage_ratio\n- high_risk_category\n- new_category_flag\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m imputer = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mtransaction_imputer.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 3. Impute missing values\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m X_imputed = \u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 4. Generate anomaly predictions\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 1 = normal, -1 = anomaly\u001b[39;00m\n\u001b[32m     39\u001b[39m pred_labels = iso_model.predict(X_imputed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\impute\\_base.py:625\u001b[39m, in \u001b[36mSimpleImputer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[32m    611\u001b[39m \n\u001b[32m    612\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \u001b[33;03m    `X` with imputed values.\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    623\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m statistics = \u001b[38;5;28mself\u001b[39m.statistics_\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m1\u001b[39m] != statistics.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\impute\\_base.py:379\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    377\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_dtype = X.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\impute\\_base.py:360\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    357\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Channel\n- CustomerAge\n- MerchantID\n- TransactionType\nFeature names seen at fit time, yet now missing:\n- category\n- category_amount_zscore\n- category_usage_ratio\n- high_risk_category\n- new_category_flag\n- ...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ISOLATION FOREST MODEL EVALUATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load preprocessed features\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X = pd.read_csv(\"transaction_features.csv\")\n",
    "print(\"Features loaded:\", X.shape)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Load trained Isolation Forest model + imputer\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "iso_model = joblib.load(\"transaction_isolation_forest_model.pkl\")\n",
    "imputer = joblib.load(\"transaction_imputer.pkl\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Impute missing values\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X_imputed = imputer.transform(X)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Generate anomaly predictions\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1 = normal, -1 = anomaly\n",
    "pred_labels = iso_model.predict(X_imputed)\n",
    "anomaly_flag = (pred_labels == -1).astype(int)\n",
    "\n",
    "# Anomaly scores (decision_function)\n",
    "scores = iso_model.decision_function(X_imputed)\n",
    "\n",
    "df_eval = pd.DataFrame(X.copy())\n",
    "df_eval[\"is_anomaly\"] = anomaly_flag\n",
    "df_eval[\"anomaly_score\"] = scores\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Basic statistics\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "total = len(df_eval)\n",
    "anomalies = df_eval[\"is_anomaly\"].sum()\n",
    "\n",
    "print(\"\\n--- ANOMALY STATISTICS ---\")\n",
    "print(f\"Total events       : {total}\")\n",
    "print(f\"Detected anomalies : {anomalies}\")\n",
    "print(f\"Anomaly %          : {anomalies/total*100:.2f}%\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Score distribution\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df_eval[\"anomaly_score\"], bins=100, kde=True)\n",
    "plt.title(\"Isolation Forest Decision Score Distribution\")\n",
    "plt.xlabel(\"Anomaly Score (higher = normal, lower = anomalous)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. Top N anomalies for inspection\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "top_n = 20\n",
    "print(f\"\\nTop {top_n} most anomalous events:\")\n",
    "print(df_eval.sort_values(\"anomaly_score\").head(top_n))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. Optional: If ground truth exists\n",
    "# ---------------------------------------------------------\n",
    "# Uncomment and use if your dataset has a true label column\n",
    "# df_eval['true_label'] = ...  # 1 = fraud/anomaly, 0 = normal\n",
    "# print(\"\\nPrecision / Recall / F1\")\n",
    "# precision = precision_score(df_eval['true_label'], df_eval['is_anomaly'])\n",
    "# recall = recall_score(df_eval['true_label'], df_eval['is_anomaly'])\n",
    "# f1 = f1_score(df_eval['true_label'], df_eval['is_anomaly'])\n",
    "# print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9. Save evaluation results\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_eval.to_csv(\"transaction_eval_results.csv\", index=False)\n",
    "print(\"\\nSaved: transaction_eval_results.csv\")\n",
    "print(\"Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ISOLATION FOREST EVALUATION: K-FOLD & STABILITY\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load preprocessed features\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(\"transaction_features.csv\")\n",
    "print(\"Loaded features:\", df.shape)\n",
    "\n",
    "# Load imputer\n",
    "imputer = joblib.load(\"transaction_imputer.pkl\")\n",
    "X = imputer.transform(df)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Evaluation parameters\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "n_splits = 5          # 5-fold\n",
    "n_estimators = 350\n",
    "contamination = 0.03\n",
    "random_state = 42\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Store results\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. K-Fold Isolation Forest Training & Evaluation\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(f\"\\nRunning {n_splits}-Fold Stability Evaluation...\")\n",
    "\n",
    "fold_idx = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    \n",
    "    model = IsolationForest(\n",
    "        n_estimators=n_estimators,\n",
    "        contamination=contamination,\n",
    "        max_samples=\"auto\",\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies on test fold\n",
    "    pred_labels = model.predict(X_test)\n",
    "    anomaly_flag = (pred_labels == -1).astype(int)\n",
    "    scores = model.decision_function(X_test)\n",
    "    \n",
    "    all_labels.append(anomaly_flag)\n",
    "    all_scores.append(scores)\n",
    "    \n",
    "    print(f\"Fold {fold_idx}: Anomalies detected = {anomaly_flag.sum()} / {len(anomaly_flag)}\")\n",
    "    fold_idx += 1\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Analyze Stability Across Folds\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "all_labels_arr = np.concatenate(all_labels)\n",
    "all_scores_arr = np.concatenate(all_scores)\n",
    "\n",
    "print(\"\\n--- Overall Statistics ---\")\n",
    "print(f\"Total samples evaluated: {len(all_labels_arr)}\")\n",
    "print(f\"Total anomalies detected: {all_labels_arr.sum()}\")\n",
    "print(f\"Anomaly percentage: {all_labels_arr.sum() / len(all_labels_arr) * 100:.2f}%\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Score distribution visualization\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(all_scores_arr, bins=100, kde=True)\n",
    "plt.title(\"Isolation Forest Decision Score Distribution (All Folds)\")\n",
    "plt.xlabel(\"Anomaly Score (higher = normal, lower = anomalous)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Optional: Stability metric (Jaccard similarity)\n",
    "# ---------------------------------------------------------\n",
    "# # Measure how consistent anomalies are across folds\n",
    "\n",
    "# from itertools import combinations\n",
    "\n",
    "# def jaccard_similarity(a, b):\n",
    "#     return np.sum(a & b) / np.sum(a | b)\n",
    "\n",
    "# similarities = []\n",
    "\n",
    "# for fold_a, fold_b in combinations(all_labels, 2):\n",
    "#     similarities.append(jaccard_similarity(fold_a.astype(bool), fold_b.astype(bool)))\n",
    "\n",
    "# print(\"\\nAnomaly detection stability (Jaccard similarity) across folds:\")\n",
    "# print(f\"Mean similarity: {np.mean(similarities):.3f}\")\n",
    "# print(f\"Min similarity : {np.min(similarities):.3f}\")\n",
    "# print(f\"Max similarity : {np.max(similarities):.3f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. Save evaluation results\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"anomaly_flag\": all_labels_arr,\n",
    "    \"anomaly_score\": all_scores_arr\n",
    "})\n",
    "\n",
    "eval_df.to_csv(\"transaction_model_kfold_eval.csv\", index=False)\n",
    "print(\"\\nSaved: transaction_model_kfold_eval.csv\")\n",
    "print(\"K-Fold Stability Evaluation COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e65eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
